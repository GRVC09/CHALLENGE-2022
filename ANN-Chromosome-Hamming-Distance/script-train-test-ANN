#!/bin/bash
#In this script we describe the methodology that is used to perform from the data processing to the training of the artificial neural network.
#The softwares where the code were prove are python2.17 and tensorflow 0.11

gunzip *.vcf.gz				#Unzip all *.gz documents in *.vcf files

echo 'Convert *.vcf -> csv'
python conver_vcf_csv.py		#Convert *.vcf to *.csv files

echo 'Processing csv files per continent'
python pre-data-set.py			#This algorithm separated the .csv files for known continents (EUR.dat,AMR.dat...) or unknown (UNK.dat) 
					#from the coordination.txt.
					#Also, create the Information.txt file, where we have the description of the sample for any continent 
					#or unknown reference and information about how many reads are per chromosome.

mkdir All_old_files			#Move all the files that we are not use anymore.
mv *.csv All_old_files/
mv *.vcf All_old_files/

echo 'Processing files'
python make_continent_datas.py		#At this point, we will use the make_continent_datas.py algorithm to get the data organized in a way  
					#that makes it easy to calculate the Hamming distance with the hamming-distance.py algorithm.
echo 'Compute Hamming distance'		#The making_in_ANN.py generates the proper input format for tensorflow.
python hamming-distance.py		
					


python ANN.py				#At the end, we feed the ANN with the processing datas and save the model in the .meta file

echo 'Predicting the population for each sample and save the results in Prediction.txt'
python ANN_prediction.py		#To predict the ancestry of each sample, we use the ANN_prediction where the .meta file 
					#contains all the values for weights and bias.

python predict-values.py		#Finally, write the name of the sample reference, the expected continent and the prediction in the Prediction.txt.
