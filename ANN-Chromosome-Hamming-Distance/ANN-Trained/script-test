#!/bin/bash
#In this script, we use a pre-trained artificial neural network to predict at least TWO samples; those samples can be in *.gz,*.vtf or *.csv format.
#The softwares where the code were prove are python2.17 and tensorflow 0.11

gunzip *.vcf.gz				#Unzip all *.gz documents in *.vcf files

python conver_vcf_csv.py		#Convert *.vcf to *.csv files

python pre-data-set.py			#This algorithm separated the .csv files for known continents (EUR.dat,AMR.dat...) or unknown (UNK.dat) 
					#from the coordination.txt.
					#Also, create the Information.txt file, where we have the description of the sample for any continent 
					#or unknown reference and information about how many reads are per chromosome.


python make_continent_datas.py		#At this point, we will use the make_continent_datas.py algorithm to get the data organized in a way  
python hamming-distance.py		#that makes it easy to calculate the Hamming distance with the hamming-distance.py algorithm.
					#The making_in_ANN.py generates the proper input format for tensorflow.


					#At the end, we feed the ANN with the processing datas and save the model in the .meta file
python ANN_single_prediction.py		#To predict the ancestry of each sample, we use the ANN_prediction where the .meta file 
					#contains all the values for weights and bias. The classification.dat store the prediction for each sample
					#in order with organization in Information.txt.
